<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Stolen from Sergey and Jon Barron */
        /* with significant help from Debidatta Dwibedi's webpage */
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus, a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body, td, th {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;


            font-size: 14px
        }

        strongred {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            color: 'red'

            font-size: 14px
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            */ font-size: 15px;
            font-weight: 700
        }
    </style>
    <!-- <link rel="icon" type="image/png" href="seal_icon.png"> -->
    <script type="text/javascript" src="hidebib.js"></script>
    <title>Shashank Tripathi</title>
    <meta name="Shashank Tripathi&#39;s CMU Homepage" http-equiv="Content-Type"
          content="Shashank Tripathi&#39;s Homepage">

    <link href="css" rel="stylesheet" type="text/css">
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-90857215-1', 'auto');
        ga('send', 'pageview');

    </script>
</head>

<body>

<table width="960" border="0" align="center" cellspacing="0" cellpadding="20">
    <tbody>
    <tr>
        <td>
            <p align="center"><font size="7">Shashank Tripathi</font><br>
            </p>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="67%" valign="middle" align="justify">


                        <p>I am a PhD student (2021-) at the Max Planck Institute for Intelligent Systems where I am
                            advised by MPI Director <a href="https://ps.is.mpg.de/person/black"> Michael Black</a>.
                            Earlier, I worked as an Applied Scientist at Amazon (2019-2021). I earned my Masters
                            (2017-2019) from the Robotics Institute, Carnegie Mellon University, working with Prof.t <a
                                    href="http://www.cs.cmu.edu/~kkitani/"> Kris Kitani</a>
                            <br>
                        <p>At Amazon Lab126, I closely collaborated with Prof. <a href="https://rehg.org/"> James
                            Rehg</a>, Dr. <a href="https://scholar.google.com/citations?user=Q3puGtcAAAAJ&hl=en"> Amit
                            Agrawal</a> and Dr. <a href="https://scholar.google.com/citations?user=GaSWCoUAAAAJ&hl=en">
                            Ambrish Tyagi</a>. It has been my great fortune to have worked with excellent mentors and
                            advisors.
                            <br>

                        </p>
                        <p align="center">
                            <a href="mailto:shashank.tripathi123@gmail.com">Email</a> &nbsp;/&nbsp;
                            <!-- <a href="assets/DebidattaDwibedi_CV.pdf">CV</a> &nbsp;/&nbsp; -->
                            <a href="https://scholar.google.com/citations?user=CANstcsAAAAJ&hl=en">Google Scholar</a>
                            &nbsp/&nbsp
                            <a href="https://github.com/sha2nkt"> GitHub</a>&nbsp;/&nbsp;
                            <a href="https://www.linkedin.com/in/shashanktripathi123/"> LinkedIn </a> /
                            <a href="assets/CV_Shashank_LongPhD_V28.pdf"> CV </a>
                            <!-- <a href="https://www.twitter.com/debidatta/"> Twitter </a> -->
                        </p>

                        <!--
                      </p><p align="center">
                        <a href="#publications">Publications</a> &nbsp;/&nbsp;

                        <a href="#patents">Patents</a> &nbsp/&nbsp
                        <a href="#talks">Talks</a>&nbsp;/&nbsp;
                        <a href="#theses">Theses</a>&nbsp;/&nbsp;
                        <a href="#misc">Misc</a>
                      </p>
                        -->


                    </td>
                    <td width="33%"><img src="assets/shashank.jpg" width="90%"
                                         style="background-repeat: no-repeat;background-position: 50%;border-radius: 50%;width: 200px;height: 200px;">
                    </td>
                </tr>
                </tbody>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">


                <tbody>
                <tr>
                    <td>
                        <h2>Research</h2>
                        My research lies at the intesection of machine-learning, computer vision and computer graphics.
                        Specifically, I am interested in 3D modelling of human bodies with limited supervision. I am
                        also working on understanding human motion in 3D. In the past, I have worked on synthetic data
                        for applications like object detection and human pose estimation.
                        <br/><br/>
                        In the past, I have worked on visual-servoing, medical-image analysis, pedestrian-detection and
                        reinforcement learning.

                    </td>
                </tr>

                </tbody>
            </table>


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

                <tbody>
                <tr>
                    <td>
                        <h2 id="publications">Publications</h2></td>
                </tr>

                <tr>
                    <td style="padding:20px;width:20%;vertical-align:middle">
                        <a href="assets/pose_combined.gif">
                            <img src="assets/pose_combined.gif" width="200"></a>
                    </td>
                    <td width="58%" valign="top">
                        <p><a href="https://cvml.page.link/pose" id="PoseNet3D">
                            <heading>PoseNet3D: Unsupervised 3D Human Shape and Pose Estimation</heading>
                        </a><br>
                            Shashank Tripathi, <a href="https://www.siddhantranade.com/">Siddhant Ranade</a>, Ambrish
                            Tyagi and Amit Agrawal<br>
                            <em><a href="http://cvpr2020.thecvf.com/">Computer Vision and Pattern Recognition (CVPR)
                                2020 (submitted) </a></href> </em>
                            <!-- <em>(under submission)</a></href> </em> -->
                            <br>
                            <br>
                            Temporally consistent recovery of 3D human pose from 2D joints without using 3D data in any
                            form<br>

                        </p>

                        <div class="paper" id="posenet3d">
                            <a href="https://arxiv.org/abs/2003.03473">paper</a> |
                            <a href="javascript:toggleblock(&#39;posenet3d_abs&#39;)">abstract</a> |
                            <a href="https://www.youtube.com/playlist?list=PL46Tof4i1hsHK6D-y8oBSRK-DpmvfyVlf">videos</a>
                            <!-- <a shape="rect" href="javascript:togglebib(&#39;terse&#39;)" class="togglebib">bibtex</a> | -->
                            <!-- <a href="assets/poster_v5_final_print.pdf">poster</a> -->

                            <!-- <a href="https://sites.google.com/corp/view/actionablerepresentations/"> project </a> -->


                            <p align="justify"><i id="posenet3d_abs" style="display: none;"> Recovering 3D human pose
                                from 2D joints is a highly unconstrained problem. We propose a novel neural network
                                architecture, PoseNet3D, that takes 2D joints as input and outputs 3D skeletons and SMPL
                                pose parameters. By casting our learning approach in a Knowledge Distillation framework,
                                we avoid using any 3D data such as paired 2D-3D data, unpaired 3D data, motion capture
                                sequences or multi-view images during training. We first train a teacher network that
                                outputs 3D skeletons, using only 2D poses for training. The teacher network distills its
                                knowledge to a student network that predicts 3D pose in SMPL representation. Finally,
                                both the teacher and the student networks are jointly fine tuned in an end-to-end manner
                                using self-consistency and adversarial losses, improving the accuracy of the individual
                                networks. Results on Human3.6M dataset for 3D human pose estimation demonstrate that our
                                approach reduces the 3D joint prediction error by 18% or more compared to previous
                                methods. Qualitative results show that the recovered 3D poses and meshes are natural,
                                realistic, and flow smoothly over consecutive frames. </i></p>

                            <div style="white-space: pre-wrap; display: none;" class="bib">
                                @inproceedings{tripathi2019learning,
                                title={Learning to generate synthetic data via compositing},
                                author={Tripathi, Shashank and Chandra, Siddhartha and Agrawal, Amit and Tyagi, Ambrish
                                and Rehg, James M and Chari, Visesh},
                                booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern
                                Recognition},
                                pages={461--470},
                                year={2019}
                                }
                            </div>
                        </div>
                    </td>
                </tr>

                <tr>
                    <td width="42%" valign="top"><a href="assets/terse_teaser.png"><img src="assets/terse_teaser.png"
                                                                                        width="100%" height="80%"
                                                                                        style="border-style: none"></a>
                    </td>
                    <td width="58%" valign="top">
                        <p><a href="https://arxiv.org/abs/1904.05475" id="TERSE">
                            <heading>Learning to Generate Synthetic Data via Compositing</heading>
                        </a><br>
                            Shashank Tripathi, <a href="https://siddharthachandra.github.io/">Siddhartha Chandra</a>,
                            Amit Agrawal, Ambrish Tyagi, <a href="https://rehg.org/"> James Rehg</a> and Visesh
                            Chari<br>
                            <em><a href="http://cvpr2019.thecvf.com/">Computer Vision and Pattern Recognition (CVPR)
                                2019 </a></href> </em>
                            <br>
                            <br>
                            Efficient, task-aware and realisitic synthesis of composite images for training
                            classification and object detection models<br>

                        </p>

                        <div class="paper" id="terse">
                            <a href="https://arxiv.org/abs/1904.05475">paper</a> |
                            <a href="javascript:toggleblock(&#39;terse_abs&#39;)">abstract</a> |
                            <a shape="rect" href="javascript:togglebib(&#39;terse&#39;)" class="togglebib">bibtex</a> |
                            <a href="assets/poster_v5_final_print.pdf">poster</a>

                            <!-- <a href="https://sites.google.com/corp/view/actionablerepresentations/"> project </a> -->


                            <p align="justify"><i id="terse_abs" style="display: none;"> We present a task-aware
                                approach to synthetic data generation. Our framework employs a trainable synthesizer
                                network that is optimized to produce meaningful training samples by assessing the
                                strengths and weaknesses of a `target' network. The synthesizer and target networks are
                                trained in an adversarial manner wherein each network is updated with a goal to outdo
                                the other. Additionally, we ensure the synthesizer generates realistic data by pairing
                                it with a discriminator trained on real-world images. Further, to make the target
                                classifier invariant to blending artefacts, we introduce these artefacts to background
                                regions of the training images so the target does not over-fit to them.
                                We demonstrate the efficacy of our approach by applying it to different target networks
                                including a classification network on AffNIST, and two object detection networks (SSD,
                                Faster-RCNN) on different datasets. On the AffNIST benchmark, our approach is able to
                                surpass the baseline results with just half the training examples. On the VOC person
                                detection benchmark, we show improvements of up to 2.7% as a result of our data
                                augmentation. Similarly on the GMU detection benchmark, we report a performance boost of
                                3.5% in mAP over the baseline method, outperforming the previous state of the art
                                approaches by up to 7.5% on specific categories. </i></p>

                            <div style="white-space: pre-wrap; display: none;" class="bib">
                                @inproceedings{tripathi2019learning,
                                title={Learning to generate synthetic data via compositing},
                                author={Tripathi, Shashank and Chandra, Siddhartha and Agrawal, Amit and Tyagi, Ambrish
                                and Rehg, James M and Chari, Visesh},
                                booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern
                                Recognition},
                                pages={461--470},
                                year={2019}
                                }
                            </div>
                        </div>
                    </td>
                </tr>

                <tr>
                    <td width="42%" valign="top"><a href="assets/c2f_img.PNG"><img src="assets/c2f_img.PNG" width="90%"
                                                                                   height="30%"
                                                                                   style="border-style: none"></a>
                    </td>
                    <td width="58%" valign="top">
                        <p>
                            <a href="https://www.eurekaselect.com/node/159218/article/c2f-coarse-to-fine-vision-control-system-for-automated-microassembly"
                               id="C2F">
                                <heading>C2F: Coarse-to-Fine Vision Control System for Automated Microassembly</heading>
                            </a><br>
                            Shashank Tripathi, Devesh Jain and Himanshu Dutt Sharma<br>
                            <em><a href="https://benthamscience.com/journals/nanoscience-and-nanotechnology-asia/">Nanotechnology
                                and Nanoscience-Asia 2018 </a></href> </em>
                            <br>
                            <br>
                            Automated, visual-servoing based closed loop system to perform 3D micromanipulation and
                            microassembly tasks<br>

                        </p>

                        <div class="paper" id="c2f">
                            <a href="https://drive.google.com/file/d/0B0keZAN_TLL9VmZ2dlAxRlYzYjA/view">paper</a> |
                            <a href="javascript:toggleblock(&#39;c2f_abs&#39;)">abstract</a> |
                            <a href="https://www.youtube.com/watch?v=lAagBmqj_Nw">video</a>


                            <!-- <a shape="rect" href="javascript:togglebib(&#39;c2f&#39;)" class="togglebib">bibtex</a> -->


                            <p align="justify"><i id="c2f_abs" style="display: none;">In this paper, authors present the
                                development of a completely automated system to perform 3D micromanipulation and
                                microassembly tasks. The microassembly workstation consists of a 3 degree-of-freedom
                                (DOF) MM3A® micromanipulator arm attached to a microgripper, two 2 DOF PI® linear
                                micromotion stages, one optical microscope coupled with a CCD image sensor, and two CMOS
                                cameras for coarse vision. The whole control strategy is subdivided into sequential
                                vision based routines: manipulator detection and coarse alignment, autofocus and fine
                                alignment of microgripper, target object detection, and performing the required assembly
                                tasks. A section comparing various objective functions useful in the autofocusing regime
                                is included. The control system is built entirely in the image frame, eliminating the
                                need for system calibration, hence improving speed of operation. A micromanipulation
                                experiment performing pick- and-place of a micromesh is illustrated. This demonstrates a
                                three-fold reduction in setup and run time for fundamental micromanipulation tasks, as
                                compared to manual operation. Accuracy, repeatability and reliability of the programmed
                                system is analyzed.</i></p>

                            <!-- <div style="white-space: pre-wrap; display: none;" class="bib">
                              @article{dwibedi2016deep,
                              title={Deep cuboid detection: Beyond 2d bounding boxes},
                              author={Dwibedi, Debidatta and Malisiewicz, Tomasz and Badrinarayanan, Vijay and Rabinovich, Andrew},
                              journal={arXiv preprint arXiv:1611.10010},
                              year={2016}
                            } -->
                        </div>
                        </div>
                    </td>
                </tr>

                <tr>
                    <td width="42%" valign="top"><a href="assets/isbi_image.png"><img src="assets/isbi_image.PNG"
                                                                                      width="100%" height="40%"
                                                                                      style="border-style: none"></a>
                    </td>
                    <td width="58%" valign="top">
                        <p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950682" id="ALZHEIMERS">
                            <heading>Sub-cortical Shape Morphology and Voxel-based Features for Alzheimer's Disease
                                Classification
                            </heading>
                        </a><br>
                            Shashank Tripathi, Seyed Hossein Nozadi, <a
                                    href="https://www.researchgate.net/profile/Mahsa_Shakeri2">Mahsa Shakeri</a> and <a
                                    href="http://www.polymtl.ca/expertises/en/kadoury-samuel">Samuel Kadoury</a><br>
                            <em><a href="https://biomedicalimaging.org/2017/">IEEE International Symposium on Biomedical
                                Imaging (ISBI) 2017 </a></href> </em>
                            <br>
                            <br>
                            Alzheimer's disease patient classification using a combination of grey-matter voxel-based
                            intensity variations and 3D structural (shape) features extracted from MRI brain scans <br>

                        </p>

                        <div class="paper" id="sub_cortical">
                            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950682">paper</a> |
                            <a href="javascript:toggleblock(&#39;sub_cortical_abs&#39;)">abstract</a> |
                            <a shape="rect" href="javascript:togglebib(&#39;sub_cortical&#39;)"
                               class="togglebib">bibtex</a> |
                            <a href="assets/sub_cortical_poster.pdf">poster</a>

                            <!-- <a href="https://sites.google.com/corp/view/actionablerepresentations/"> project </a> -->


                            <p align="justify"><i id="sub_cortical_abs" style="display: none;"> Neurodegenerative
                                pathologies, such as Alzheimer’s disease, are linked with morphological alterations and
                                tissue variations in subcortical structures which can be assessed from medical imaging
                                and biological data. In this work, we present an unsupervised framework for the
                                classification of Alzheimer’s disease (AD) patients, stratifying patients into four
                                diagnostic groups, namely: AD, early Mild Cognitive Impairment (MCI), late MCI and
                                normal controls by combining shape and voxel-based features from 12 sub-cortical areas.
                                An automated anatomical labeling using an atlas-based segmentation approach is proposed
                                to extract multiple regions of interest known to be linked with AD progression. We take
                                advantage of gray-matter voxel-based intensity variations and structural alterations
                                extracted with a spherical harmonics framework to learn the discriminative features
                                between multiple diagnostic classes. The proposed method is validated on 600 patients
                                from the ADNI database by training binary SVM classifiers of dimensionality reduced
                                features, using both linear and RBF kernels. Results show near state-of-the-art
                                approaches in classification accuracy (>88%), especially for the more challenging
                                discrimination tasks: AD vs. LMCI (76.81%), NC vs. EMCI (75.46%) and EMCI vs. LMCI
                                (70.95%). By combining multimodality features, this pipeline demonstrates the potential
                                by exploiting complementary features to improve cognitive assessment. </i></p>

                            <div style="white-space: pre-wrap; display: none;" class="bib">
                                @inproceedings{tripathi2017sub,
                                title={Sub-cortical shape morphology and voxel-based features for Alzheimer's disease
                                classification},
                                author={Tripathi, Shashank and Nozadi, Seyed Hossein and Shakeri, Mahsa and Kadoury,
                                Samuel},
                                booktitle={Biomedical Imaging (ISBI 2017), 2017 IEEE 14th International Symposium on},
                                pages={991--994},
                                year={2017},
                                organization={IEEE}
                                }
                            </div>
                        </div>
                    </td>
                </tr>

                <tr>
                    <td width="42%" valign="top"><a href="assets/miccai_img.png"><img src="assets/miccai_img.png"
                                                                                      width="100%" height="15%"
                                                                                      style="border-style: none"></a>
                    </td>
                    <td width="58%" valign="top">
                        <p><a href="https://link.springer.com/chapter/10.1007/978-3-319-51237-2_2" id="DEEP_SPECTRAL">
                            <heading>Deep Spectral-Based Shape Features for Alzheimer’s Disease Classification</heading>
                        </a><br>
                            <a href="https://www.researchgate.net/profile/Mahsa_Shakeri2">Mahsa Shakeri</a>, <a
                                    href="https://profs.etsmtl.ca/hlombaert/">Hervé Lombaert</a>, Shashank Tripathi and
                            <a href="http://www.polymtl.ca/expertises/en/kadoury-samuel">Samuel Kadoury</a><br>
                            <em><a href="http://www.miccai2016.org/en/">MICCAI Spectral and Shape Analysis in Medical
                                Imaging (SeSAMI) 2016</a></em>
                            <br>
                            <br>
                            Alzheimer's disease classification by using deep learning variational auto-encoder on shape
                            based features<br>

                        </p>

                        <div class="paper" id="deepspectral">
                            <a href="https://link.springer.com/chapter/10.1007/978-3-319-51237-2_2">paper</a> |
                            <a href="javascript:toggleblock(&#39;deepspectral_abs&#39;)">abstract</a> |
                            <a shape="rect" href="javascript:togglebib(&#39;deepspectral&#39;)"
                               class="togglebib">bibtex</a> |
                            <!-- <a href="https://github.com/debidatta/syndata-generation">code</a> |
                            <a href="assets/cutpaste_poster.pdf">poster</a> -->


                            <p align="justify"><i id="deepspectral_abs" style="display: none;">Alzheimer’s disease (AD)
                                and mild cognitive impairment (MCI) are the most prevalent neurodegenerative brain
                                diseases in elderly population. Recent studies on medical imaging and biological data
                                have shown morphological alterations of subcortical structures in patients with these
                                pathologies. In this work, we take advantage of these structural deformations for
                                classification purposes. First, triangulated surface meshes are extracted from segmented
                                hippocampus structures in MRI and point-to-point correspondences are established among
                                population of surfaces using a spectral matching method. Then, a deep learning
                                variational auto-encoder is applied on the vertex coordinates of the mesh models to
                                learn the low dimensional feature representation. A multi-layer perceptrons using
                                softmax activation is trained simultaneously to classify Alzheimer’s patients from
                                normal subjects. Experiments on ADNI dataset demonstrate the potential of the proposed
                                method in classification of normal individuals from early MCI (EMCI), late MCI (LMCI),
                                and AD subjects with classification rates outperforming standard SVM based approach.</i>
                            </p>

                            <div style="white-space: pre-wrap; display: none;" class="bib">
                                @inproceedings{shakeri2016deep,
                                title={Deep spectral-based shape features for alzheimer’s disease classification},
                                author={Shakeri, Mahsa and Lombaert, Herve and Tripathi, Shashank and Kadoury, Samuel
                                and Alzheimer’s Disease Neuroimaging Initiative and others},
                                booktitle={International Workshop on Spectral and Shape Analysis in Medical Imaging},
                                pages={15--24},
                                year={2016},
                                organization={Springer}
                                }
                            </div>
                        </div>
                    </td>
                </tr>


                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                    <tr>
                        <td>
                            <h2 id="misc">Miscellaneous</h2>
                            Some other unpublished work:
                        </td>
                    </tr>
                    </tbody>
                </table>

                <table width="100%" align="center" border="0" cellpadding="20">
                    <tbody>
                    <tr>
                        <td width="42%" valign="top"><a href="assets/supercnn_img.png"><img
                                src="assets/supercnn_img.png" width="100%" height="30%" style="border-style: none"></a>
                        </td>
                        <td width="58%" valign="center">
                            <p>
                                <a href="assets/learning_salient_objects.pdf">
                                    <heading>Learning Salient Objects in a Scene using Superpixel-augmented
                                        Convolutional Neural Networks
                                    </heading>
                                </a><br>
                            </p>
                            <div class="paper" id="super_cnn">
                                <a href="assets/learning_salient_objects.pdf"> report </a> |
                                <a href="assets/SuperCNN.pdf"> slides </a> |
                                <a href="https://github.com/sha2nkt/SuperCNN"> code </a>


                            </div>

                        </td>
                    </tr>
                    <tr>
                        <td width="42%" valign="top"><a href="assets/tracking_combined.gif"><img
                                src="assets/tracking_combined.gif" width="100%" style="border-style: none"></a>

                        </td>
                        <td width="58%" valign="center">
                            <p>
                                <a href="assets/tracking.pdf">
                                    <heading>Moving object detection, tracking and classification from an unsteady
                                        camera
                                    </heading>
                                </a><br>
                            </p>
                            <div class="paper" id="tracking">
                                <a href="assets/tracking.pdf"> slides </a> |
                                <a href="https://www.youtube.com/watch?v=g_nTKhVyPHw"> video </a>


                        </td>


                    </tr>


                    <tr>
                        <td width="42%" valign="top"><a href="assets/model_combined.gif"><img
                                src="assets/model_combined.gif" width="100%" height="120%"
                                style="border-style: none"></a>
                        </td>
                        <td width="58%" valign="center">
                            <p>
                                <a href="assets/deep-rl-final.pdf">
                                    <heading>Towards integrating model dynamics for sample efficient reinforcement
                                        learning
                                    </heading>
                                </a><br>
                            </p>
                            <div class="paper" id="model_based">
                                <a href="assets/deep-rl-final.pdf"> report </a> |
                                <a href="https://github.com/sha2nkt/QD_learning"> code </a>
                            </div>

                        </td>
                    </tr>

                    </tbody>
                </table>

                </td>
                </tr>
                </tbody>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <br>
                        <p align="right"><font size="2">
                            <a href="https://www.cs.berkeley.edu/~barron/">website adapted the awesome Jon Barron</a>
                        </font></p>

                    </td>
                </tr>

            <script xml:space="preserve" language="JavaScript">
  hideallbibs();
</script>
            <script xml:space="preserve" language="JavaScript">
  hideblock('pa13_abs');
</script>
            <script xml:space="preserve" language="JavaScript">
  hideblock('cuboid_abs');
</script>
            <script xml:space="preserve" language="JavaScript">
  hideblock('mftcn_abs');
</script>
            <script xml:space="preserve" language="JavaScript">
  hideblock('temporal_abs');
</script>


</body>
</html>